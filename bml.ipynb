{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\sabni\\Documents\\BML\\ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "print(df.head(20))\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Display info and basic stats\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Check for null values\n",
    "print(\"Any Null Values Present:\")\n",
    "print(df.isnull().any())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "print(\"Sum of Null Values per Column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Any Duplicated Rows:\")\n",
    "print(df.duplicated().any())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "output_path = r\"C:\\Users\\sabni\\Documents\\SKILL\\hi2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Store encoders for inverse transformation if needed\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"NObeyesdad\"])  # Features\n",
    "y = df[\"NObeyesdad\"]  # Target variable\n",
    "\n",
    "# Split dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=500, multi_class='ovr', solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Gaussian Naïve Bayes (for continuous data)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "report_gnb = classification_report(y_test, y_pred_gnb)\n",
    "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy_gnb:.4f}\")\n",
    "print(\"Gaussian Naïve Bayes Classification Report:\\n\", report_gnb)\n",
    "\n",
    "# Multinomial Naïve Bayes (for count-based or discrete features)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "accuracy_mnb = accuracy_score(y_test, y_pred_mnb)\n",
    "report_mnb = classification_report(y_test, y_pred_mnb)\n",
    "print(f\"\\nMultinomial Naïve Bayes Accuracy: {accuracy_mnb:.4f}\")\n",
    "print(\"Multinomial Naïve Bayes Classification Report:\\n\", report_mnb)\n",
    "\n",
    "# Bernoulli Naïve Bayes (for binary/boolean features)\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "accuracy_bnb = accuracy_score(y_test, y_pred_bnb)\n",
    "report_bnb = classification_report(y_test, y_pred_bnb)\n",
    "print(f\"\\nBernoulli Naïve Bayes Accuracy: {accuracy_bnb:.4f}\")\n",
    "print(\"Bernoulli Naïve Bayes Classification Report:\\n\", report_bnb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure X_train and X_test are numeric\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Encode categorical labels (if necessary)\n",
    "if isinstance(y_train[0], str):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "class CustomGaussianNB:\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Gaussian Naïve Bayes model.\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {c: np.mean(y == c) for c in self.classes}  # P(y)\n",
    "        self.means = {c: np.mean(X[y == c], axis=0) for c in self.classes}  # μ\n",
    "        self.variances = {c: np.var(X[y == c], axis=0) + 1e-6 for c in self.classes}  # σ² (add small value to avoid division by zero)\n",
    "\n",
    "    def gaussian_pdf(self, x, mean, var):\n",
    "        \"\"\"Compute the probability density function of a Gaussian distribution.\"\"\"\n",
    "        return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.class_priors[c])  # log(P(y))\n",
    "                likelihood = np.sum(np.log(self.gaussian_pdf(x, self.means[c], self.variances[c])))  # log(P(X|y))\n",
    "                class_probs[c] = prior + likelihood  # log(P(y|X))\n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train Custom Gaussian Naïve Bayes\n",
    "nb_custom = CustomGaussianNB()\n",
    "nb_custom.fit(X_train.to_numpy(), y_train)  # Convert to NumPy array\n",
    "\n",
    "# Predictions\n",
    "y_pred_custom = nb_custom.predict(X_test.to_numpy())  # Convert to NumPy array\n",
    "\n",
    "# Evaluation\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "report_custom = classification_report(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"Custom Gaussian Naïve Bayes Accuracy: {accuracy_custom:.4f}\")\n",
    "print(\"Custom Gaussian Naïve Bayes Classification Report:\\n\", report_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class CustomMultinomialNB:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y)  # Convert to NumPy array for proper indexing\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # Compute class priors: P(y)\n",
    "        self.class_priors = {c: np.mean(y == c) for c in self.classes}\n",
    "        \n",
    "        # Compute feature counts (Laplace smoothed)\n",
    "        self.feature_counts = {c: np.sum(X[y == c], axis=0) + self.alpha for c in self.classes}\n",
    "        \n",
    "        # Compute feature probabilities: P(X|y)\n",
    "        self.feature_probs = {c: self.feature_counts[c] / self.feature_counts[c].sum(axis=0, keepdims=True) for c in self.classes}\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.class_priors[c])  # log(P(y))\n",
    "                \n",
    "                # Avoid log(0) issues by replacing 0 with a small value\n",
    "                feature_prob = np.clip(self.feature_probs[c], 1e-10, None)\n",
    "                \n",
    "                likelihood = np.sum(np.log(feature_prob) * x)  # log(P(X|y))\n",
    "                class_probs[c] = prior + likelihood  # log(P(y|X))\n",
    "            \n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train Custom Multinomial Naïve Bayes\n",
    "nb_multinomial = CustomMultinomialNB(alpha=1)\n",
    "nb_multinomial.fit(X_train.to_numpy(), y_train.to_numpy())  # Convert to NumPy\n",
    "\n",
    "# Predictions\n",
    "y_pred_multinomial = nb_multinomial.predict(X_test.to_numpy())  # Convert to NumPy\n",
    "\n",
    "# Evaluation\n",
    "accuracy_multinomial = accuracy_score(y_test, y_pred_multinomial)\n",
    "report_multinomial = classification_report(y_test, y_pred_multinomial)\n",
    "\n",
    "print(f\"\\nCustom Multinomial Naïve Bayes Accuracy: {accuracy_multinomial:.4f}\")\n",
    "print(\"Custom Multinomial Naïve Bayes Classification Report:\\n\", report_multinomial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure numeric X and encode categorical y\n",
    "X_train, X_test = X_train.apply(pd.to_numeric, errors='coerce'), X_test.apply(pd.to_numeric, errors='coerce')\n",
    "if isinstance(y_train[0], str):\n",
    "    le = LabelEncoder()\n",
    "    y_train, y_test = le.fit_transform(y_train), le.transform(y_test)\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=5, criterion=\"gini\"):\n",
    "        self.max_depth, self.criterion, self.tree = max_depth, criterion, None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        probs = np.bincount(y) / len(y)\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))  # Avoid log(0)\n",
    "\n",
    "    def _gini(self, y):\n",
    "        probs = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(probs**2)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_score = None, None, float('inf')\n",
    "        score_fn = self._gini if self.criterion == \"gini\" else self._entropy\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            for threshold in np.unique(X[:, feature]):\n",
    "                left, right = y[X[:, feature] <= threshold], y[X[:, feature] > threshold]\n",
    "                if len(left) == 0 or len(right) == 0: continue\n",
    "                weighted_score = (len(left) * score_fn(left) + len(right) * score_fn(right)) / len(y)\n",
    "                if weighted_score < best_score:\n",
    "                    best_feature, best_threshold, best_score = feature, threshold, weighted_score\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if len(np.unique(y)) == 1 or depth >= self.max_depth:\n",
    "            return np.bincount(y).argmax()\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        if best_feature is None: return np.bincount(y).argmax()\n",
    "\n",
    "        mask = X[:, best_feature] <= best_threshold\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \n",
    "                \"left\": self._build_tree(X[mask], y[mask], depth+1),\n",
    "                \"right\": self._build_tree(X[~mask], y[~mask], depth+1)}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_one(self, x, tree):\n",
    "        return self._predict_one(x, tree[\"left\"]) if isinstance(tree, dict) and x[tree[\"feature\"]] <= tree[\"threshold\"] else (self._predict_one(x, tree[\"right\"]) if isinstance(tree, dict) else tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "\n",
    "# Train and Evaluate\n",
    "tree_model = CustomDecisionTree(max_depth=5, criterion=\"gini\")\n",
    "tree_model.fit(X_train.to_numpy(), y_train)\n",
    "y_pred_tree = tree_model.predict(X_test.to_numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tree):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Decision Tree model\n",
    "tree_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=5, random_state=42)  \n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "report_tree = classification_report(y_test, y_pred_tree)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_tree:.4f}\")\n",
    "print(\"Decision Tree Classification Report:\\n\", report_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train KNN Model\n",
    "k = 5  # Choose an appropriate value of k\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "report_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_knn:.4f}\")\n",
    "print(\"KNN Classification Report:\\n\", report_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure X_train and X_test are numeric\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Encode categorical labels (if necessary)\n",
    "if isinstance(y_train[0], str):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "class CustomKNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k  # Number of neighbors\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"Calculate Euclidean distance manually\"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))  # Vectorized for better performance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data\"\"\"\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        \"\"\"Predict a single sample\"\"\"\n",
    "        # Compute distances from the test sample to all training samples\n",
    "        distances = np.array([self.euclidean_distance(x, x_train) for x_train in self.X_train])\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_neighbors_idx = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get the labels of k nearest neighbors\n",
    "        k_neighbors_labels = self.y_train[k_neighbors_idx]\n",
    "        \n",
    "        # Return the most common label\n",
    "        return Counter(k_neighbors_labels).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multiple samples\"\"\"\n",
    "        return np.array([self.predict_one(x) for x in X])\n",
    "\n",
    "# Train Custom KNN Model\n",
    "k = 5  # Choose k value\n",
    "knn_custom = CustomKNN(k=k)\n",
    "knn_custom.fit(X_train.to_numpy(), y_train)  # Convert to NumPy array\n",
    "\n",
    "# Predictions\n",
    "y_pred_custom_knn = knn_custom.predict(X_test.to_numpy())  # Convert to NumPy array\n",
    "\n",
    "# Evaluation\n",
    "accuracy_custom_knn = accuracy_score(y_test, y_pred_custom_knn)\n",
    "report_custom_knn = classification_report(y_test, y_pred_custom_knn)\n",
    "\n",
    "print(f\"Custom KNN Accuracy: {accuracy_custom_knn:.4f}\")\n",
    "print(\"Custom KNN Classification Report:\\n\", report_custom_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data for better clustering performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)  # Scale training data\n",
    "\n",
    "# Train K-Means Model\n",
    "k = 3  # Number of clusters\n",
    "kmeans_model = KMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=42)\n",
    "kmeans_model.fit(X_scaled)\n",
    "\n",
    "# Cluster Assignments\n",
    "clusters = kmeans_model.predict(X_test)\n",
    "\n",
    "# Centroid Locations\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "\n",
    "print(f\"Cluster Assignments:\\n{clusters}\")\n",
    "print(f\"Centroids:\\n{centroids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CustomKMeans:\n",
    "    def __init__(self, k=3, max_iters=100, tol=1e-4):\n",
    "        self.k = k  # Number of clusters\n",
    "        self.max_iters = max_iters  # Maximum number of iterations\n",
    "        self.tol = tol  # Tolerance for convergence\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"Calculate Euclidean distance manually (Vectorized)\"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"Randomly initialize k centroids\"\"\"\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.choice(len(X), self.k, replace=False)\n",
    "        return X[indices]\n",
    "\n",
    "    def assign_clusters(self, X, centroids):\n",
    "        \"\"\"Assign each data point to the nearest centroid\"\"\"\n",
    "        distances = np.array([[self.euclidean_distance(x, centroid) for centroid in centroids] for x in X])\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update_centroids(self, X, clusters):\n",
    "        \"\"\"Update centroids by computing the mean of assigned points\"\"\"\n",
    "        new_centroids = np.array([X[clusters == i].mean(axis=0) if np.any(clusters == i) else self.centroids[i] for i in range(self.k)])\n",
    "        return new_centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train the K-Means model\"\"\"\n",
    "        X = np.array(X)  # Ensure it's a NumPy array\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            clusters = self.assign_clusters(X, self.centroids)\n",
    "            new_centroids = self.update_centroids(X, clusters)\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.max(np.abs(new_centroids - self.centroids)) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        self.clusters = clusters  # Store final clusters\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Assign new data points to the nearest cluster\"\"\"\n",
    "        return self.assign_clusters(X, self.centroids)\n",
    "\n",
    "\n",
    "# Load and Normalize Data (if needed)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train.to_numpy())  # Convert to NumPy array and normalize\n",
    "\n",
    "# Train Custom K-Means Model\n",
    "k = 3  # Number of clusters\n",
    "kmeans_custom = CustomKMeans(k=k)\n",
    "kmeans_custom.fit(X_scaled)\n",
    "\n",
    "# Predictions\n",
    "clusters = kmeans_custom.predict(X_test.to_numpy())  # Convert to NumPy array\n",
    "\n",
    "print(f\"Cluster Assignments:\\n{clusters}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
